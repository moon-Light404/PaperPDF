数据拥有者只允许这些数据保存在自己手中，进而会形成各自独立的数据孤岛，未来在不同组织之间进行数据整合工作是十分有挑战性的。

 

有两个或两个以上的联邦学习参与方协作构建一个共享的机器学习模型。每一个参与方都拥有若干个能够用来训练模型的训练数据。数据不离开拥有者

 

联邦学习模型的性能要能够充分逼近理想模型(指通过所有训练数据集中在一起并训练获得的机器学习模型)的性能。

 

**横向联邦学习**

1、 各参与方在本地计算模型精度，并使用同态加密等加密技术，对梯度信息进行掩饰，并将掩饰后的结果(加密梯度)发送给服务器进行聚合

2、 服务器进行安全聚合操作，如使用基于同态加密的加权平均

3、 服务器将聚合后的结果发送给各参与方

4、 各参与方对收到的梯度进行解密，并使用解密后的梯度结果更新各自的模型参数

![image-20230219001927644](https://cdn.jsdelivr.net/gh/moon-Light404/my_picgo@master/img/image-20230219001927644.png)

上述步骤持续迭代运行，直到损失函数收敛或者达到允许的迭代次数上限或允许的训练时间，这种架构独立于特定的机器学习算法，并且所有参与方将会共享最终的模型参数。

 

除了共享梯度信息，参与方还可以共享模型的参数，参与方在本地计算模型参数，并将它们发送到服务器，服务器对收到的模型参数进行聚合，再将聚合的模型参数发送给参与方，这种方法为模型平均。

 

 

 

 

 

 

 

<u>衡量模型的预测能力的好坏</u>

损失函数：是定义在单个训练样本上的，也就是就算一个样本的误差，比如我们想要分类，就是预测的类别和实际类别的区别，是一个样本的哦，用L表示。

代价函数：是定义在整个训练集上面的，也就是**所有样本的误差的总和的平均，也就是损失函数的总和的平均**，有没有这个平均其实不会影响最后的参数的求解结果。

 

<u>FedAvg算法</u>  适用于有限加和形式的损失函数

 ![image-20230219001839607](https://cdn.jsdelivr.net/gh/moon-Light404/my_picgo@master/img/image-20230219001839607.png)

![image-20230219012734389](https://cdn.jsdelivr.net/gh/moon-Light404/my_picgo@master/img/image-20230219012734389.png)

C:比例系数，每次选取客户端的比例,选取部分客户端进行训练并上传

E:每一轮中每个客户机投入其全部本地训练集训练一遍的次数。本地全部数据训练一次叫做一个epoch。

其实就是min_batch，把样本分成若干个大小相同的子集，针对每个子集只做**一次梯度下降**，更新w和b的值，，遍历完所有的min_batch，其实也就相当于选取了部分的数据集进行梯度下降，而full_batch是对所有的样本进行计算。

每个batch大小为B，分成了 $\beta$ 组，也就要遍历 $\beta$ 次，每次只对每组进行一次梯度下降(更新$w$)，若 B =  $\infty$ 表示 $batch$ 为全部样本，此时就是 $full-batch$ 梯度下降了。

 

当 $E = 1, B = \infty$ 时，对于的就是 **FEDSGD**，即每一次客户机一次性将所有本地数据投入训练，更新参数模型。

  **FedSGD：每次采用client的所有数据集进行训练，本地训练次数为1，然后进行aggregation。**



**<u>联邦学习的另一种实现形式</u>**

![image-20230306232847651](https://cdn.jsdelivr.net/gh/moon-Light404/my_picgo@master/img/image-20230306232847651.png)







#### trimmed_mean算法

[拜占庭容错机器学习算法之剪枝平均_byzantine-robust distributed learning: towards opt_咸鱼菲菲的博客-CSDN博客](https://blog.csdn.net/watqw/article/details/124778831)