**信息量**
事件发生的概率越低，该事件发生对应的信息量就越高，反之发生概率越高的事件信息量越低。
信息量公式：$I(x_0) = -log_2P(x)$

**熵**
熵越大，代表着这个系统的不确定性越高、混乱程度越大，也可以表示为无损编码事件信息的最小平均编码长度。**信息熵**用于衡量整个事件空间包含的平均信息量，为信息的平均期望。
$$Entropy =-\sum_i P(i)log_2 P(i) -最小平均编码长度 $$其中，$P(i)$是第$i$个状态的可能性。
在信息论中，熵（Entropy）是衡量随机变量不确定性的一个度量。对于一个离散概率分布 $P$，其熵 $H(P)$ 定义为：

$$
H(P) = -\sum_{i} P(x_i) \log_b P(x_i)
$$

其中，$x_i$ 表示可能的输出，$P(x_i)$ 表示输出 $x_i$ 的概率，$\log_b$ 是以 $b$ 为底的对数函数。在信息论中，常常使用以 2 为底的对数，这时候熵的单位是比特（bit）。

如果随机变量 $X$ 的概率分布为 $P$，那么 $X$ 的熵记作 $H(X)$，并且有 $H(X) = H(P)$。

这个公式的含义是，如果你要用最短的编码来编码一个随机变量，那么**平均每个符号需要的比特数**就是这个随机变量的熵。

---



**交叉熵**
交叉熵可以看作是把来自一个分布$q$的消息使用另一个分布$p$的最佳编码传达的平均长度，或者某个分布的信息量在另一个分布的信息熵。
$H(p,q)=\sum_xq(x)log_2()\frac{1}{p(x)}=-\sum_xq(x)log_2p(x)$

二分类：
激活函数为 sigmmod $\rightarrow f(z)=\frac{1}{1+e^{-z}}$

损失函数为$L(\mathbf{w})=-\frac{1}{N}\sum_{i=1}^N[y_ilogf(x_i)+(1-y_i)log(1-f(x_i))]$

简写成$$C=-\frac{1}{N}\sum_{i=1}^N[y_ilna+(1-y_i)ln(1-a)]$$
其中$z=wx+b,a=\sigma(z)$,$N$表示样本数量。


多分类：
激活函数为softmax $$\sigma(\mathbf{z})_j=\frac{e^{z_j}}{\sum_{k=1}^Ke^{z_k}}$$

损失函数为：
$$C=-\sum_{i=1}^k y_i \ lna$$
其中$a=\sigma(z)$.