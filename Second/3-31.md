## Defending Batch-level Label Inference and Replacement Attacks in Vertical Federated Learnin

本文证明了即使在HE保护的VFL中间梯度通信中，梯度反转攻击仍然可以高精度地重建私有标签，这与通常认为批平均信息在加密下可以安全共享的观点相反。后门攻击也可以通过直接替换加密的通信消息而不进行解密来进行。为了解决这些攻击，提出了一种新的防御方法，混淆自动编码器(称为CAE)，它基于自编码器和熵正则化来伪装真实标签。为了进一步防御具有足够先验标签知识的攻击者，我们引入了离散tesgd增强的CAE(称为DCAE)，并表明在防御各种标签推理攻击时，DCAE比其他已知方法显著提高了主任务准确性。

<div align="center" size="30%">
<img width=600px height=atuo src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image.png">
</img>
</div>
<!-- <div align="center" >
<img width=600px  height=auto src="">
</img>
</div> -->

把有标签的称为active party，其余的称为passive party，为了进一步防止样本级梯度信息在中间结果中泄露，对每个样本的通信更新 $\frac{\partial l}{\partial H_i}$应用同态加密，记作$[[\cdot]]$，并在加密梯度下进行计算。采取以下步骤：
1)协调器创建加密对，向各方发送公钥;2)各方对梯度计算的中间结果进行加密和通信;3)双方分别计算加密梯度并添加附加掩码，然后将加密值发送给协调器;4)协调器解密并将解密后的梯度返回给各方;5)各方揭开梯度，更新相应的模型参数。

### 训练过程

联合训练问题:$o_i^K=f(\theta_1,\theta_2,\cdots,\theta_K;\mathbf{x}_i^1,\cdots,\mathbf{x}_i^K)$
$\theta_i$分别是参与方$i$的本地模型，$x_i^K$是参与方的本地所拥有的特征对应的数据。

优化问题：
$$min_{\Theta}\mathcal{L}(\Theta,D)=\frac{1}{N}\sum_{i=1}^N\mathbb{l}(o_i^K,y_i^K)+\beta\sum_{k=1}^K\gamma(\theta_k)$$
$N$表示所有总样本的数量，每个参与方本地训练一个子模型$H_i^K=G_k(\theta_k,x_i^K)$生成本地预测，最后的预测是通过使用非线性函数(如sigmod)合并$H_i^k$，也就是

$$l(\theta_1,\cdot,\theta_K;D_i)=l(S(\sum_{k=1}^KH_i^k),y_i^K)$$
让$H_i=\sum_{k=1}^K H_i^k$，对于样本$i$的梯度计算有:$\nabla_k l(\theta_1,\cdots,\theta_K;D_i)=\frac{\partial l}{\partial H_i}\frac{\partial H_i^k}{\partial \theta_k}$

### 方法

<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-6.png">
</img>
</div>

(i)攻击者可以访问被动一方的训练数据，并控制被动一方的训练过程和局部模型;(ii)攻击者只接收纯文本的批处理平均局部梯度和加密的每样本局部梯度;(iii)攻击者不修改VFL协议中的训练权重和梯度等局部更新;(iv)攻击者不控制任何良性方的训练，也不控制全局通信和聚合协议。

**DIL Label Inference**
$\frac{\partial l}{\partial H_i}$是一个$N$维向量，$N$是类别的数量，设$j-th$元素：

<div align="center" >
<img width=300px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-1.png">
</img>
</div>

$S_j$是softmax函数$S_j=\frac{e^{h_j}}{\sum_v e^{h_v}}$，这里我们滥用符号$y$来表示真标签的索引。如果$\frac{\partial l}{\partial H_i}$被攻击者发现并知道，则标签信息是已知的，因为$\frac{\partial l}{\partial H_i}$的第$y$个元素将是唯一与其他元素符号相反的元素，从而泄露真实标签。

$H_i^k$是参与方k的本地模型的logits位，是$C$维向量，$C$是分类任务的类别数，$B$表示批大小$B$，总损失$l_B=\frac{1}{B}\sum_{i=1}^Bl_i$，设客户端$k$上最后一个完全连通层参数为$\theta_{k,o}(C \times M)$，其中$M$是特征嵌入的维度，最后一层的局部梯度为：
<div align="center" >
<img width=300px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-2.png">
</img>
</div>

从左到右的矩阵维度分别为$C \times M,C \times B,B \times M$。
<div align="center" >
<img width=500px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-3.png">
</img>
</div>

结论：当$B \leq rank(A) \leq M$，标签可以100%推断。理论上是上述的批处理梯度可以转换成多个线性方程组，未知数对应的是$\frac{\partial l_B}{\partial H_B}$，每个样本$i$相对于$H_i$的梯度。

#### 梯度反演攻击

<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-4.png">
</img>
</div>

具体来说，被被动方(攻击者)p建立一个内部模型，对批B中的每个样本i尝试猜测标签，并传达中间结果$\{H_i^a\}_{i \in B}$，使模拟的局部梯度与观测值相匹配。首先，对于每个批次B中的每个样本$i$，随机初始化$y_i'$和$H_i^{a'}$，然后计算梯度:
<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-5.png">
</img>
</div>

优化上述目标函数，匹配虚拟梯度，从而恢复出$y_i$。

### LABEL REPLACEMENT BACKDOOR ATTACKS

这个后门任务是建立在前面讨论的标签推理攻击的基础上的，因为一旦被动方学习到每个样本对应的真标签，它就可以仔细设计加密的通信消息，用目标标签替换真标签。经过标签推理，得到$\frac{\partial l}{\partial H_i}$，$\frac{\partial l}{\partial H_i}$的第y个元素有和其他位置的元素相反的符号。
<div align="center" >
<img width=300px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-7.png">
</img>
</div>

这里我们用 y 表示真实标签的序列，如果$g_H^i$被揭露了，那么标签的信息就知道了，因为$g_H^i$的第y个元素与其他的相比有完全不同的标识。为了实施攻击，被动方只需要用以下式子代替即可：
<div align="center" >
<img width=300px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-8.png">
</img>
</div>

然而，这种后门攻击需要标签推理步骤作为先决条件，这需要首先分别运行和观察 VFL 过程。因此，作者提出了一种新的后门攻击：
<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-9.png">
</img>
</div>

假设被动攻击者知道一些干净的样本，标签与后门任务的目标标签相同，把这个样本集记为$D_{target}$攻击主要操作如下：

- 在前向传播过程中，攻击者在本地计算$H_i^k$，对于每个毒性样本$i$，随机地从$D_{target}$选择一个样本$j$，用$[[H_i^k]]i \in D_{boakdoor}$代替$[[H_j^k]]j \in D_{target}$作为中间加密的结果，并记录$\lang i,j\rang$对。
- 在后向传播中，对于每个$\lang i,j\rang$对，攻击者将收到的样本i的加密中间梯度替换成样本j的加密梯度。

#### 防御

作者提出了一种简单有效的标签伪装技术，即主动方学着把原始标签转换为一个“soft fake labels”集。为了增加混淆，使用一个混淆自编码器(CoAE)来建立一个映射，这样一个标签将以每个选项类概率都高的被转换成一个软标签。例如，一只狗被映射为狗和猫的概率分别为[0.5, 0.5]。

<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-10.png">
</img>
</div>

编码器把真实标签$y$作为输入，输出假的标签$\~y$；解码器输入$\~y$，输出重建的原始标签$\^y$，引入了一个比较损失和熵损失：
<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-11.png">
</img>
</div>

主动方可以利用CAE产生假标签，并使用这些假标签计算VFL中的梯度，从而防止标签泄漏攻击。在执行推理时，活动方使用训练好的解码器将预测的标签转换回来。本文最后还给出了DCAE方案，即使用离散DiscreteSGD方法进行梯度离散进行防御。
