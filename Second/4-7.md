## 本周小结
主要看了一些针对VFL中标签推理攻击的防御方法，包括MARVELL、Max-Norm、PELoss、dCorr这几类，需要继续深入了解一下。

### 《Making Split Learning Resilient to Label Leakage by Potential Energy Loss》

在本文中，我们关注由训练好的分割模型引起的隐私泄露问题，即攻击者可以使用少量标记样本来微调底层模型，并获得相当好的性能。为了防止这种隐私泄露，我们提出了潜在的能量损失，通过将同一类别的输出推向决策边界，使底层模型的输出变得更加“复杂”的分布。因此，对手在仅使用少量泄漏的标记样本对底部模型进行微调时会遭受很大的泛化误差。实验结果表明，我们的方法显着降低了攻击者的微调精度，使分割模型对标签泄漏更具弹性。
- 我们提出底部模型输出的潜在能量损失，以将其推至决策边界
- 我们根据攻击者的泛化误差形式化了底层模型的隐私泄漏，并证明使数据点位于决策边界附近有助于保护隐私。


> We observe that such distribution can be obtained if the data points of the same class are distributed near the boundary of the decision region/我们观察到，如果同一类的数据点分布在决策区域边界附近，就可以获得这样的分布


借鉴了物理学中静电平衡的思想，同种电荷（同号电荷）相互吸引，异性电荷相互排斥。受此启发，我们可以将同一类的数据点视为同类电荷，并且彼此之间具有排斥力。结果，这些数据点往往会彼此远离并被推到决策区域的边界。
<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image12.png">
</img>
</div>


其中 C 是标签集，Hc 是 c 标记样本的底部模型输出。“By adding Lpe to the loss function, during the training of the split model, the bottom model outputs of the same class are pushed away from each other, and move towards the boundary of the decision region of the top model.” 通过在损失函数中加入Lpe，在分割模型的训练过程中，将同一类的底层模型输出相互推开，并向顶层模型的决策区域边界移动。  

待深入了解...偏该文章理论。

----


### 《Label Leakage and Protection in Two-party Split Learning》

在这项工作中，我们探讨了一方在分割训练期间是否有可能窃取另一方的私有标签信息，以及是否有方法可以防止此类攻击。具体来说，我们首先制定一个现实的威胁模型，并提出一个隐私损失指标来量化分割学习中的标签泄漏。然后我们证明，威胁模型中存在两种简单而有效的方法，可以让一方准确地恢复另一方拥有的私有地面实况标签。为了对抗这些攻击，我们提出了几种随机扰动技术，包括 Marvell，这种方法通过最小化最坏情况对手的标签泄漏量（通过我们的量化指标测量）来战略性地找到噪声扰动的结构。我们凭经验证明了我们的保护技术针对已识别攻击的有效性，并表明 Marvell 相对于基准方法尤其改进了隐私与实用性权衡。
<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-13.png">
</img>
</div>




主要贡献：
 1.在二元分类的背景下形式化了两方分割学习中标签泄漏的威胁模型（第 3.1 节），并提出了具体的隐私量化指标来衡量此类威胁的严重性（第 3.2 节）。 
 2.在该威胁模型中确定了两种简单而现实的方法，可以准确地恢复标签方的自有标签信息（第3.3节）。 
 1. 提出了几种随机扰动技术来限制非标签方的标签窃取能力（第 4 节）。其中，我们的原则方法 Marvell 直接搜索最佳随机扰动噪声结构，以最大限度地减少针对最坏情况的对抗性非标签方的标签泄漏（通过我们的量化指标测量）。

#### Threat Model and Privacy Quantification
本文考虑了二分类的隐私度量，量化评分函数$r$的性能指标为ROC曲线的AUC分数。
$$TFP=\frac{TP}{P}=\frac{TP}{TP+FN}$$
$$FPR=\frac{FP}{N}=\frac{FP}{FP+TN}$$

AUC值可以看作TPR-FPR(ROC曲线)与$x$轴围成的面积，也可以看成是积分。

$$AUC(r)=\int_{\infty}^{-\infty}TPR_r(t)\ dFPR_r(t) \in [0,1]$$
该分数越高，说明在评分函数$r$上隐私泄露程度越大。

#### Practical Attack Methods

**Norm-based scoring function** 
观察到$\|g\|_2=|\~p_1-y| \cdot \|\nabla_ah(a)|_{a=f(X)}\|_2$。
1、在整个训练过程中，模型对于正例是正例的信心往往低于负例是负例的信心。换句话说，正观察 1 的置信度差距：在整个训练过程中，模型对于正例为正的信心往往低于负例为负的信心。换句话说，正例的置信度差距 $1-\~p_1=|\~p_1-y|$ （当 y = 1 时）通常大于负例的置信度差距$1-\~p_0=\~p_1=|\~p_1-y|$ （当 y = 0 时）
2、在整个训练过程中，正例和负例的梯度向量 $‖∇zh(z)|_{z=f(X)}‖_2$ 的范数处于相同的数量级（具有相似的分布）（图 2（ b））。这是很自然的，因为 $∇_ah(a)|_{a=f(X)}$ 不是 y 的函数。

**结论**：正实例的梯度范数 $‖g‖_2$ 通常大于负实例的梯度范数（图 2(c)）。因此，评分函数 rn(g) = ‖g‖2 是未见标签 y 的强预测因子。我们将针对攻击测量的隐私损失（泄漏 AUC）命名为规范泄漏 AUC 。

**Direction-based scoring function**
证明了$g$的方向可以泄露标签，对于一对示例$(X_a,y_a),(X_b,y_b)$，它们各自的预测正类概率为$\~p_{1,a},\~p_{1,b}$，它们传递的梯度为$g_a,g_b$，余弦相似度
$$cos(g_a,g_b)=g_a^Tg_b/(\|g_1\|_2\|g_b\|_2)=sgn(\~p_{a,1}-y_a)\cdot sgn(\~p{1,b-y_b})\cdot cos(\nabla_zh(z)|_{z=f(X_a)},\nabla_zh(z)|_{z=f(X_b)})$$

1、当示例a和b属于不同类别时，$sgn(\~p_{a,1}-y_a)\cdot sgn(\~p{1,b-y_b})=-1$，a和b属于同类别时，式子等于1.
2、在整个训练过程中，对于任意两个示例 $a、b$，它们的函数 $h$ 的梯度始终形成锐角，即 $cos(∇_zh(z)|z=f(Xa), ∇_zh(z)|z= f(Xb)) > 0$（图 2(d)）。对于使用单调递增激活函数（例如 ReLU、sigmoid、tanh）的神经网络，这是由于这些激活函数相对于其输入的梯度是坐标非负的，因此始终位于第一个闭超正交领域中。
**结论**：观察 3 和 4 中项的乘积，我们看到对于给定的示例，同一类的所有示例都会导致正余弦相似度，而所有相反类的示例都会导致负余弦相似度。如果问题是类不平衡的，并且无标签方知道正例比负例少，那么它可以确定每个例子的标签。

#### Label Leakage Protection Methods
**Norm-alignment heuristic**
一种改进的启发式方法，添加具有非各向同性和示例相关协方差的零均值高斯噪声。 
[幅度选择] 正如我们所看到的，正例和负例的 $‖g‖2$ 可能不同，从而泄漏标签信息，这种启发式的第一个目标是使每个扰动梯度的范数彼此无法区分。具体来说，我们希望将小批量中每个扰动梯度的预期平方 $2-范数$与该批次中最大的平方 2-范数（表示为 $‖gmax‖_2^ 2$）相匹配。

[方向选择] 此外，正如我们从图2（e）经验中看到的，正梯度和负梯度位于Rd中的一维线附近，正样本指向一个方向，负样本指向另一个方向。因此，我们只考虑沿着“这条线”添加噪声（粗略地说）。更具体地说，对于批次中的梯度 $g_j$，我们添加仅在沿 $g_j$ 线的一维空间上支持的零均值高斯噪声向量 $η_j$。换句话说，噪声的协方差是1阶矩阵 $Cov[η_j] = σ_j^2 g_jg^T_j$ 。为了计算 $σj$，我们的目标是匹配 $E[‖g_j + η_j ‖_2^2] = ‖g_{max}‖_2^2$。简单计算得出 $σ_j = ‖g_{max}‖_2^2/‖g_j ‖_2^2 − 1$。因为我们对齐最大范数，我们将这种启发式保护方法命名为 max_norm。 max_norm 的优点是它没有需要调整的参数。不幸的是，它没有很强的理论动机，不能灵活地权衡模型效用和隐私，并且可能会被一些未知的攻击所破坏。

**Optimized Perturbation Method: Marvell**
目标，最小化泄露AUC指标分数：
$$min_{D^{(1)},D^{(0)}}max_rAUC(r)$$
<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-14.png">
</img>
</div>

----

### 《Label Leakage and Protection from Forward Embedding in Vertical Federated Learning》

由于原始标签通常包含高度敏感的信息，因此最近提出了一些工作来有效防止 vFL 中反向传播梯度的标签泄漏。然而，这些工作只是识别并防御了反向传播梯度的标签泄漏威胁。这些工作都没有关注中间嵌入的标签泄漏问题。在本文中，我们提出了一种实用的标签推断方法，即使应用了一些现有的保护方法（例如标签差分隐私和梯度扰动），该方法也可以从共享中间嵌入中有效地窃取私有标签。标签攻击的有效性与中间嵌入和相应私有标签之间的相关性密不可分。为了减轻前向嵌入的标签泄漏问题，我们在标签方添加了一个额外的优化目标，通过最小化中间嵌入和相应私有标签之间的距离相关性来限制对手的标签窃取能力。我们进行了大量实验来证明我们提出的保护方法的有效性。

#### Defense
距离相关性可用于测量任意维度（不一定相等）的两对随机向量之间的统计相关性[17]。它可以测量两个向量之间的线性和非线性关联[21]。另一个优点是它可以优化并轻松融入传统的优化框架（例如最小化交叉熵损失）。

给定来自一小批量数据中的$f(X)$和$Y$，使用$dCor(f(X),Y)$来衡量$f(X)$和$Y$之间的独立性，其中$f(X)\in \mathbb{R}^{n\times d}$，$Y \in \mathbb{R}^{n \times 1}$
<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-15.png">
</img>
</div>
<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-16.png">
</img>
</div>
<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-17.png">
</img>
</div>
与训练结合，在标签方计算的总体损失函数是两个损失函数的结合，距离相关损失($L_d$)和正常标签预测损失($L_c$)：
$$\mathcal{L}=\mathcal{L}_C+\alpha_d\mathcal{L}_d$$
