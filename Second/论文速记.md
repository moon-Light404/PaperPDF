

## 标题
### CAFE: <a href="https://dblp.org/search?q=vertical%20federated%20learning%20%7C%20inference">Catastrophic Data Leakage in Vertical Federated Learning</a>, 出处：NeurlPS 2021CCFA

### 摘要
&emsp;&emsp;最近的研究表明，私人训练数据可以通过分布式机器学习系统（例如联邦学习（FL））中部署的梯度共享机制泄露。增加批量大小以使数据恢复复杂化通常被视为一种有前途的数据泄漏防御策略。在本文中，我们重新审视了这一防御前提，并提出了一种具有理论依据的高级数据泄漏攻击，以有效地从共享聚合梯度中恢复批量数据。我们将我们提出的方法命名为垂直联合学习（CAFE）中的灾难性数据泄漏。与现有的数据泄漏攻击相比，我们在垂直 FL 设置上的大量实验结果证明了 CAFE 执行大批量数据泄漏攻击的有效性，并提高了数据恢复质量。我们还提出了缓解 CAFE 的实际对策。我们的结果表明，参与标准 FL 的私有数据，尤其是垂直案例，从训练梯度中泄露的风险很高。我们的分析表明这些学习环境中存在前所未有的实际数据泄露风险。我们的工作代码可在 https://github.com/DeRafael/CAFE 获取。

### 介绍
&emsp;&emsp;联邦学习 (FL) [8, 24] 是一种新兴的机器学习框架，其中中央服务器和多个工作人员协作训练机器学习模型。一些现有的 FL 方法考虑的设置是，每个工作人员都拥有一组不同主题的数据，但具有共同特征。此设置也称为数据分区或水平 FL (HFL)。与 HFL 设置不同，在许多学习场景中，多个工作人员处理同一组主题的数据，但每个工作人员都有一组不同的特征。这种情况在金融和医疗保健应用中很常见[6]。在这些示例中，数据所有者（例如金融机构和医院）在其联合用户库中拥有这些用户的不同记录，因此，通过 FL 结合他们的特征，他们可以建立更准确的模型。我们将此设置称为特征分区或垂直 FL (VFL)。与现有的分布式学习范式相比，FL提出了包括数据异构性和隐私性在内的新挑战[20]。为了保护数据隐私，服务器和工作人员之间仅交换模型参数和参数的变化（例如梯度）[19, 15]。最近的工作研究了恶意工作者如何在 FL 中嵌入后门或替换全局模型 [2,3,27]。此外，由于交换梯度通常被视为隐私保护协议，因此很少关注公共共享梯度和批次身份的信息泄漏。在数据安全和人工智能伦理的背景下，从 FL 梯度推断私人用户数据的可能性越来越受到人们的关注 [10, 14, 21]，这被称为数据泄漏问题。

&emsp;&emsp;之前的工作已经在通过梯度进行数据恢复方面做出了探索性的努力。详细信息请参见第 2 节和表 1。然而，现有的方法通常在扩大大批量数据恢复方面存在局限性，并且缺乏数据恢复能力的理论依据，这可能会给人一种错误的安全感，认为在训练期间增加数据批量大小可以防止数据泄漏。 30]。最近的一些工作为保证数据恢复提供了充分的条件，但假设过于严格，有时可能不切实际，例如要求类的数量远大于恢复数据样本的数量[29]。
&emsp;&emsp;为了增强数据恢复的可扩展性并获得对 VFL 中数据泄漏的基本了解，在本文中，我们提出了一种高级数据泄漏攻击，并对数据恢复性能进行了理论分析，我们将其称为垂直联邦学习中的灾难性数据泄漏（CAFE） ）。作为说明，图 1 展示了与现有方法相比，CAFE 在大批量数据恢复方面的有效性。本文的贡献总结如下：

<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image24.png">
</img>
</div>

C1）我们开发了一种名为 CAFE 的新数据泄漏攻击，以克服当前 VFL 数据泄漏攻击的局限性。利用 VFL 中数据索引和内部表示对齐的新颖用法，CAFE 能够恢复通用 VFL 协议中的大规模数据。 

C2）我们为CAFE的恢复性能提供了理论保证，这贯穿了CAFE的三个步骤：（I）恢复相对于第一个全连接（FC）层输出的损失梯度； (II)恢复第一FC层的输入； (三)恢复原始数据。 
C3）为了减轻 CAFE 的数据泄漏攻击，我们开发了一种利用虚假梯度并保持模型训练性能的防御策略。 
C4) 我们对静态和动态 VFL 训练设置进行了大量实验，以验证 CAFE 相对于最先进方法的卓越数据恢复性能。

#### 相关工作
&emsp;&emsp;从梯度中恢复私人训练数据引起了 FL 越来越多的兴趣。最近，一种流行的方法被称为梯度深度泄漏（DLG）[32]，它可以在不使用任何生成模型或先验信息的情况下以有效的方式推断训练数据。然而，DLG缺乏模型架构和权重分布初始化的通用性[25]。在[30]中，开发了一种分析方法来从梯度中提取准确的标签。在[11]中，开发了另一种分析方法来导出全连接（FC）层之前的输入。然而，在[11]中，他们的方法仅适用于单个样本输入，无法扩展一批数据。在[22]中，开发了一种新方法，通过求解线性方程来恢复FC层之前的批量输入。然而，为求解方程做出了强有力的假设，并且不能保证在更一般情况下的数据恢复。在[9]中，声称卷积层总是可以转换为FC层。然而，原始卷积层的梯度仍然与转换后的FC层的梯度不同，这阻碍了数据恢复。除了[11]中提出的新损失函数之外，之前的一些工作还基于DLG设计了新的损失函数或正则化器，并尝试使他们的算法适用于更通用的模型和权重分布初始化。在[25]中，使用基于梯度差的新高斯核作为距离度量。在[31]中，开发了一种递归方法攻击程序来从梯度中恢复数据。然而，在[25]和[31]中，批量数据的恢复质量都下降了。最近的一项工作 [29] 提出了一种名为 GradInversion 的算法，用于根据给定的梯度从噪声中重建图像。然而，他们的理论和算法大多建立在强有力的假设和经验观察的基础上。尽管他们成功重建了一批训练数据，但报告的批量大小仍然不大于 48。


### 总结
&emsp;&emsp;在本文中，我们通过一种新颖的算法揭示了垂直联邦学习（CAFE）中灾难性数据泄漏的风险，该算法可以执行大批量数据泄漏，并且具有高数据恢复质量和理论保证。大量的实验结果表明，CAFE可以从垂直FL设置上的共享聚合梯度中恢复大规模私有数据，克服了当前数据泄漏攻击中的批量限制问题。我们还提出了一种使用假梯度的有效对策来减轻 CAFE 的潜在风险。

---


## 标题 
### HashVFL: Defending Against Data Reconstruction Attacks in Vertical Federated Learning(CCFA)

### 摘要
&emsp;&emsp;垂直联邦学习(VFL)是一种趋势协同机器学习模型训练解决方案。现有的工业框架采用同态加密等安全多方计算技术来保证数据安全和隐私。尽管做出了这些努力，但研究表明，由于中间表示和原始数据之间的相关性，数据泄漏仍然是 VFL 的风险。神经网络可以准确地捕获这些相关性，允许对手重建数据。这强调了继续研究保护 VFL 系统的必要性。我们的工作表明，散列是解决数据重建攻击的一种有前途的解决方案。散列的单向性质使得对手很难从哈希码中恢复数据。然而，在 VFL 中实现散列带来了新的挑战，包括梯度消失和信息丢失。为了解决这些问题，我们提出了 HashVFL，它集成了散列，同时实现了可学习性、位平衡和一致性。实验结果表明，HashVFL 在防御数据重建攻击的同时有效地保持了任务性能。它还为减少标签泄漏程度、减轻对抗性攻击和检测异常输入带来了额外的好处。我们希望我们的工作能够激发对 HashVFL 的潜在应用的进一步研究。

### 介绍
&emsp;&emsp;机器学习算法，特别是深度神经网络 (DNN)，近几十年来取得了重大进展 [1]、[2]、[3]。DNN已被应用于金融[4]、[5]、生物医学[6]、[7]，甚至军事行动[8]、[9]。然而，数据安全和隐私在这些敏感领域至关重要，GDPR[10]和CCPA[11]等严格的法律法规限制了数据流。这造成了机器学习模型中大量数据的需求和数据流的限制之间的困境。

&emsp;&emsp;垂直联邦学习(VFL)[12]，[13]，[14]，[15]是一个趋势范式，它解决了共享相同用户组但特征不同的公司所面临的共同困境。这个概念如图1所示。假设一个银行B方需要改进其贷款审批预测，并要求来自电子商务公司A方的附加信息。在 VFL 中，每一方不是直接交换用户数据，而是将底部模型计算的用户特征的中间结果上传到中性方以进行进一步处理。这样，原始数据仍然是保密的。VFL的主要挑战是确保这些中间结果的隐私和安全性。当前的框架采用安全多方计算(SMC)方法，如同态加密(HE)[16]，[17]来提供这些保证。HE允许在加密环境中执行计算，确保没有人可以访问纯文本中的中间结果。
&emsp;&emsp;然而，最近的研究表明这种方法是不够的。特别是[18]，[19]，[20]，[21]的研究表明，对手可以通过使用样本的后验和VFL模型的参数来重建中间结果，甚至目标方的原始数据。这些成功的数据重建攻击背后的原因是深度神经网络 (DNN) 对中间计算和原始输入之间的相关性进行建模的能力。例如，生成对抗网络 (GAN) [22]、[23] 在图像重建方面取得了显着成功。研究人员研究了几种方法[24]、[25]来防御VFL中的数据重构攻击，如增加输入与相应激活之间的相关性距离。这些方法确实减少了信息泄漏，但并不完全。在最坏的情况下，对手知道目标样本[26]的模型和输出，重建攻击仍然有机会。
&emsp;&emsp;为了消除可逆性，我们提出了一种新的VFL框架，称为HashVFL，该框架使用散列。散列的单向性质允许我们的框架阻止所有尝试从哈希码中恢复中间计算或原始输入。然而，哈希的集成使得模型在训练期间难以学习，因为梯度消失。此外，散列丢弃信息并保留隐私，必须最小化哈希码的长度。这两个因素将不可避免地影响模型的性能。鉴于上述考虑，我们的HashVFL框架的设计解决了以下三个挑战：
- 可学习性。挑战在于通过散列平衡保护隐私和确保模型的可学习性之间的权衡。解决方案是识别具有易于估计梯度的哈希函数，以便模型可以继续训练和保持其性能。我们的解决方案：为了在隐私保护和可学习性之间实现所需的权衡，我们使用以下方法：1）我们添加了一个符号函数来对每一方的中间计算进行二值化。这是哈希[27]和二元神经网络[28]中常用的技术。2)我们在反向传播中采用了直通估计器[29]、[30]。这允许梯度完全通过 Sign 函数，因为它是避免梯度消失。
- 比特平衡。散列导致每个比特的信息丢失。此外，为了最大限度地减少信息泄漏的风险，需要限制哈希码的长度。为了应对这一挑战，我们引入了比特平衡的概念。这是指在给定有限的哈希码长度的情况下最大化每个比特携带的信息。理想情况下，我们的目标是一半的样本在每个位取 1/-1 的值。这最大化了整个哈希码可以携带的信息。我们的解决方案:为了解决上述要求，我们建议使用批处理归一化(BN)[31]。BN对一批样本中每个维度的中间计算进行归一化，使其具有标准正态分布。这意味着批次中大约一半的样本将具有正值，一半将在每个维度上具有负值。因此，结合BN有助于解决比特有效性的问题。
- 一致性。由于二值化将中间结果映射到相同的潜在空间，直观地说，来自不同方的样本哈希码应该是一致的。这样做的一种方法是通过比较各方之间的哈希码差异来增加训练中的约束。然而，如果有多方，这种方法可能会导致较高的计算开销，这肯定会限制VFL的应用。我们的解决方案：为了在比较各方样本的哈希码时解决高计算开销，解决方案是为每个类预定义一组二进制代码 [32]。这样，每一方只需要将他们的哈希码与这些二进制代码进行比较，将复杂度从 O(N 2) 降低到 O(N )，其中 Ni 是参与方的数量，使其适用于多参与方场景。此外，样本和目标二进制码之间的计算差异也可以指导优化，如[33]所示。

&emsp;&emsp;实验结果表明，我们提出的 HashVFL 在各种数据类型中保持了主要任务的性能。此外，HashVFL通过减少标签泄漏、减轻对抗性攻击和检测异常输入提供了额外的优势。此外，我们评估了 HashVFL 在不同条件下的性能，例如不同数量的参与方，并发现它有效地处理各种场景。

我们的贡献如下:
- 我们提出了一种通过整合哈希技术来增强VFL中的数据安全和隐私的新方法。
- 我们在基于哈希的VFL框架的设计中解决了三个关键挑战，即可学习性、位平衡和一致性，并提出了一个实用的解决方案。
- 我们对我们的框架HashVFL进行了全面的实证评估，证明了它在防御数据重构攻击方面易于使用、多功能性和有效性。

### 总结
&emsp;&emsp;这项工作引入了 HashVFL，这是一种新的 VFL 框架，它利用散列来防御数据重建攻击。据我们所知，这是第一个包含散列的 VFL 框架。我们解决了将散列集成到VFL中的三个挑战，并提供有效的解决方案。我们的评估结果表明，HashVFL 在有效保护数据重建攻击的同时保留了主要任务的性能。此外，我们通过实验表明，HashVFL可以减少标签泄漏的程度，减轻对抗性攻击，检测异常输入。我们预计这项工作将引发对 HashVFL 实际应用的进一步研究。

---

## 标题
### Beyond model splitting: Preventing label inference attacks in vertical federated learning with dispersed training(CCF B) 出处：WWW

### 摘要
&emsp;&emsp;摘要联邦学习是一种新兴的范式，它使多个组织能够联合训练模型，而不会泄露他们的私有数据。作为一个重要的变体，垂直联邦学习 (VFL) 处理协作组织拥有同一组用户的数据但具有不相交的特征的情况。通常认为VFL比水平联邦学习更安全。然而，最近的研究(USENIX Security&#39;22)表明，在VFL中进行标签推理攻击仍然是可能的，攻击者可以获取其他参与者的私有拥有标签;即使采用模型分裂构建的VFL(安全保证较高的VFL架构类型)也不能逃脱它。为了解决这个问题，在本文中，我们提出了分散的训练框架。它利用秘密共享来打破底层模型和训练数据之间的相关性。因此，即使攻击者在训练阶段接收到梯度，他也无法从底层模型中推断出标签的特征表示。此外，我们设计了一种定制的模型聚合方法，使共享模型可以私下组合，秘密共享方案的线性保证了要保留的训练精度。理论和实验分析表明我们的框架具有令人满意的性能和有效性。

### 介绍
&emsp;&emsp;机器学习在许多领域取得了巨大的成功，如决策、风险识别和疾病诊断。机器学习的广泛采用及其有效性主要归因于，除了 ML 算法的进步外，收集、存储和处理大量数据的可负担性越来越高。特别是，联合和整合来自多个来源的数据的可行性（例如，不同研究所收集的数据或存储在不同的数据中心）。然而，共享数据可能会遇到安全和隐私问题。随着对数据隐私保护的认识的提高，立法禁止共享包含公民敏感信息的数据。例如，欧盟通用数据保护条例(GDPR)[1,2]、新加坡个人数据保护法案(PDPA)[3]和美国加州消费者隐私法案(CCPA)[4]等法规。

&emsp;&emsp;在这种情况下，提出了联邦学习 (FL)，并广受欢迎。它使多个数据所有者（例如客户端或数据中心）能够协同训练 ML 模型，而不会泄露他们的私有数据。作为基本工作流程，FL 中的参与者迭代地 (i) 对其数据进行本地计算以得出某些中间结果，(ii) 使用某些加密工具隐藏中间结果，以及 (iii) 与其他参与者共享受保护的结果，直到达到最终训练结果。根据不同类型的数据划分，FL可分为水平联邦学习(HFL)和垂直联邦学习(VFL)。HFL处理数据水平划分的情况，即数据集共享相同的特征空间，但在样本空间中不同。例如，两家医院持有不同患者组（即样本空间）的医疗记录，这些患者描述了他们是否患有某种疾病（即特征空间）。相比之下，VFL 处理数据垂直分区的情况，即数据集共享相同的样本空间，但在特征空间上有所不同。VFL 的一个典型例子是，两家医院持有同一组患者的医疗记录（即 sane 样本空间），但它们中的每一个都描述了患者的医疗状况的不同方面，例如来自医院 A 的数据集记录了患者的 COVID 拭子测试，而来自医院 A 的数据集记录了患者的胸部 CT 扫描（即不同的特征空间）。

&emsp;&emsp;由于其相对对称的结构，HFL 的构建很简单。它通常是通过模型平均构建的——在每次迭代中，客户端训练一个模型几个 epoch，并将其模型发送给客户端，客户端随后聚合提交的本地模型并获得更新的全局模型。但是在 VFL 方面，尤其是用模型拆分构建的 VFL，训练过程相对复杂。在具有模型分割的 VFL 中，整个模型分为顶层模型（在服务器上保留）和几个底层模型（作为在客户端保留的特定中间状态）。训练过程通过迭代双边通信完成。客户端使用其本地数据运行他们的底层模型并将结果上传到服务器；服务器运行顶部模型来聚合参与者的输出，计算损失的梯度，并将梯度发送回每个参与者。
&emsp;&emsp;因此，HFL 的潜在攻击者有机会窥视模型所有参数的梯度，可用于推断私人信息。相比之下，VFL 的攻击者只控制联邦模型的一部分，因此只能得到不完整模型的梯度。因此，人们普遍认为 VFL 具有更高的安全保证，特别是对于通过模型拆分构建的 VFL（参与者无法访问 DNN 的最后一层，因此服务器中的标签更安全）[5]。尽管如此，最近的研究表明，对VFL[6]进行推理攻击仍然是可行的。具体来说，傅等人。 [6] 发现，从服务器发送的梯度本质上可以帮助客户端学习关于标签的良好特征表示。这种泄露的信息用作标签推理攻击的预训练模型。

<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-25.png">
</img>
</div>


&emsp;&emsp;在本文中，我们提出了分散的训练框架，以对抗这种攻击并实现 VFL 的安全性。分散训练的基本思想是利用秘密共享打破梯度与训练数据之间的相关性。如图 1 所示，参与者 B 拥有自己的数据，标签希望训练模型；他希望利用参与者 A 的数据来提高他模型的质量。参与者 A 是位置攻击者，他旨在推断参与者 B 的标签。在分散训练框架中，为参与者 A 创建了一个阴影模型（即参与者 C），参与者 B 的一部分数据与参与者 C 共享。在训练阶段，客户端（即参与者 A 和 C）使用共享数据更新其底层模型并将它们的部分输出上传到服务器。服务器聚合客户端的部分输出并训练其顶层模型；由于秘密共享方案的线性，可以有效地聚合它们的输出。服务器的训练输出也分为两部分，并分别提供 A 和 C 的每个片段，后者随后迭代地训练和更新它们的底层模型。使用这种方法，即使攻击者在训练阶段接收到梯度，他也无法从底层模型中推断出标签的特征表示。
&emsp;&emsp;本文的其余部分安排如下。第 2 节介绍了联邦学习的基本背景，并描述了如何在垂直联邦学习上进行推理攻击。我们在第 3 节中介绍了我们分散的训练框架及其构建。性能评估在第 4 节中介绍。该主题的相关工作在第 5 节中介绍，我们在第 6 节总结了本文。


<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-26.png">
</img>
</div>

### 总结
&emsp;&emsp;以前的工作表明，VFL的隐私安全也有很大的风险。VFL中的恶意参与者可以对其他参与者的标签发起推理攻击，导致严重的隐私泄露。为了解决这个问题，我们提出了一个分散的训练框架，该框架引入了一种新的底层模型，它可以通过秘密共享在恶意底层模型的训练过程中共享部分梯度，使恶意底层模型能够更好地获取标签和特征之间的关系，从而防止标签推理攻击。实验表明，分散训练可以有效防止标签推理攻击。然而，原始联邦任务的准确性也在一定程度上受到影响，只能权衡原始联邦任务准确性和攻击精度，这为我们未来的研究提供了良好的方向。未来，我们可以考虑原有的联邦性能，降低攻击精度方向的研究。

## 标题

### Label Leakage and Protection from Forward Embedding in Vertical Federated Learning

### 摘要
&emsp;&emsp;近年来，垂直联邦学习 (vFL) 受到了广泛关注，并已部署以解决数据隐私问题的机器学习问题。然而，最近的一些工作表明，即使所涉及的参与者之间只传达了前向中间嵌入（而不是原始特征）和反向传播梯度（而不是原始标签），vFL 也容易受到隐私泄露的影响。由于原始标签通常包含高度敏感的信息，最近提出了一些工作来防止标签从 vFL 中有效反向传播的梯度泄漏。然而，这些工作只识别和防御反向传播梯度中标签泄漏的威胁。这些工作都没有关注中间嵌入的标签泄漏问题。在本文中，我们提出了一种实用的标签推理方法，该方法可以有效地从共享中间嵌入窃取私有标签，即使应用了一些现有的保护方法，如标签差分隐私和梯度扰动。标签攻击的有效性与中间嵌入和相应私有标签之间的相关性不可分割。为了缓解前向嵌入的标签泄漏问题，我们在标签方添加了一个额外的优化目标，通过最小化中间嵌入和相应私有标签之间的距离相关性来限制对手的标签窃取能力。我们进行了大量实验来证明我们提出的保护方法的有效性。


### 介绍
&emsp;&emsp;vFL的详细训练过程（包括前向传递和后向梯度计算）如图 1 所示。在前向传递期间，没有标签（非标签方）的一方将中间层（切割层）输出而不是原始数据发送到带有标签（标签方）的一方，标签方完成其余的前向计算以获得训练损失。为了计算后向梯度计算阶段模型参数的梯度，标签方从其训练损失开始反向传播并计算它自己的参数的梯度。标签方还计算关于切割层输出的梯度，并将该信息发送回非标签方，以便非标签方可以使用链式法则来计算其参数的梯度。


<div align="center" >
<img width=600px  height=auto src="https://raw.githubusercontent.com/moon-Light404/read_paper_notes/master/note_image/image-27.png">
</img>
</div>

&emsp;&emsp;尽管只有切割层嵌入（而不是原始特征）和反向传播梯度（而不是原始标签）的中间计算在两方之间通信，但现有工作 [11, 16] 表明 vFL 容易受到隐私泄露的影响。例如，[11] 证明对抗性非标签方可以利用反向传播梯度的范数和方向来准确推断私有类标签（AUC 几乎为 1.0）。由于原始标签通常包含高度敏感的信息（例如，用户购买的内容（在在线广告中）或用户是否有疾病（在疾病预测中）[20]，因此了解标签泄漏的威胁及其在 vFL 中的保护尤为重要。

&emsp;&emsp;最近提出了一些工作[11，8]来防止标签泄漏。例如，[8] 利用随机响应翻转标签并使用生成的噪声版本标签来计算损失函数。他们证明了他们提出的算法可以实现标签差分隐私(DP)[6]。[11]提出添加优化的高斯噪声来扰动反向传播的梯度，以便在扰动后无法区分正梯度和负梯度。通过最小化扰动梯度之间的和 KL 散度来计算优化添加噪声量。这两种方法都可以有效地防止在 vFL 设置中从反向传播的梯度泄漏。

&emsp;&emsp;然而，除了反向传播的梯度外，私有标签也可以通过来自非标签方发送到标签方的前向切割层嵌入的对抗性方来推断。通过模型训练，可以学习切割层嵌入与私有标签具有相关性，因此用于通过对抗性方区分不同的标签。例如，对抗性非标签方可以对这些切割层嵌入进行聚类，然后根据其大小将标签分配给集群。例如，在在线广告和疾病预测等不平衡二元分类设置中，较大的集群可以分配负标签，较小的集群将获得正标签。特别是，在本文中，我们开发了一种实用的标签推理方法，该方法基于频谱攻击[18]技术来预测私有标签。我们的推理方法可以有效地从切割层嵌入窃取私有标签，即使应用了[11]和[8]提出的保护方法(因为这两种方法都被提议防止标签泄漏反向传播的梯度)。我们的标签攻击的有效性与中间嵌入和相应私有标签之间的相关性不可分割。为了缓解前向嵌入标签泄漏的问题，我们建议减少切割层嵌入和私有标签之间的学习相关性。我们通过让标签方优化一个额外的目标来实现这一目标，该目标最小化了切割层嵌入和相应私有标签之间的距离相关性 [17]。因此，对抗方不能根据切割层嵌入和私有标签之间的有限相关性来区分正标签和负标签。

&emsp;&emsp;我们将我们的贡献总结为：1）我们提出了一种实用的标签泄漏攻击，从两方分裂学习中前向嵌入； 2）我们提出了一种相应的防御，以最小化切割层嵌入和私有标签之间的距离相关性。3)我们通过实验验证了我们的攻击和防御的有效性。4)我们的工作是第一个我们知道识别和防御 vFL 中前向切割层嵌入的标签泄漏威胁的工作。


#### 相关工作
&emsp;&emsp;**拆分学习中的信息泄漏**。最近，研究人员表明，在拆分学习中，即使原始数据（特征和标签）不共享，敏感信息仍然可以从各方之间传达的梯度和中间嵌入泄露。例如，[19] 和 [16] 表明非标签方的原始特征可以从前向切割层嵌入泄露。我们通过研究标签的泄漏而不是原始特征与它们不同。此外，[11] 研究了标签泄漏问题，但泄漏源是后向梯度而不是前向嵌入。据我们所知，之前没有研究前向嵌入的标签泄漏问题的工作。

&emsp;&emsp;vFL 中的信息保护。vFL中主要有三种信息保护技术:1)安全多方计算[2]等加密方法;2)基于系统的方法，包括可信执行环境[15];3)将噪声添加到通信消息[1,14,7,5,23]。我们的保护属于第三类。在这一系列工作中，[8] 通过随机响应生成噪声标签，以实现标签差分隐私保证。[11]添加了优化的高斯噪声来扰动反向传播的梯度，以混淆正梯度和负梯度。